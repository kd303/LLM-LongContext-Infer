{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Accelerating CLIP Inference: From CPU to GPU Preprocessing\n",
    "\n",
    "This notebook demonstrates how to optimize the inference pipeline for the **OpenCLIP** model. Standard libraries like `transformers` often default to CPU-based preprocessing, which can become a bottleneck when deploying high-performance models using **NVIDIA Triton Inference Server** or **TensorRT**.\n",
    "\n",
    "### Key Technical Areas:\n",
    "1.  **Standard Pipeline:** Using `CLIPProcessor` with CPU-based preprocessing.\n",
    "2.  **GPU Acceleration:** Implementing high-speed preprocessing using `torchvision.transforms` on the GPU.\n",
    "3.  **Benchmarking:** Measuring the performance gains between CPU and GPU-based pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-title",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "First, we'll install necessary libraries. While this notebook is compatible with Google Colab, these tools are commonly used in production environments for optimized model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries for CLIP and image processing\n",
    "!pip install -q transformers pillow torch torchvision datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-title",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "We'll use a subset of the **CIFAR-10** dataset to test our OpenCLIP zero-shot classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "total_images = 50\n",
    "# Load CIFAR-10 test set\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)\n",
    "\n",
    "# Extract a subset for benchmarking\n",
    "subset_images = [dataset[i][0] for i in range(total_images)]\n",
    "subset_label_ids = [dataset[i][1] for i in range(total_images)]\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-title",
   "metadata": {},
   "source": [
    "## 3. Loading the OpenCLIP Model\n",
    "We use the `clip-vit-large-patch14` model from OpenAI via the Hugging Face `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model_id = \"openai/clip-vit-large-patch14\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bench-cpu-title",
   "metadata": {},
   "source": [
    "## 4. Standard Case: CPU-Based Preprocessing\n",
    "In a typical pipeline, the `CLIPProcessor` handles image resizing and normalization on the CPU before passing tensors to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bench-cpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_ms(start, end):\n",
    "    return (end - start) * 1000\n",
    "\n",
    "candidate_queries = [f\"a photo of a {name}\" for name in class_names]\n",
    "pre_proc_times, model_times, post_proc_times = [], [], []\n",
    "\n",
    "for i in range(len(subset_images)):\n",
    "    image = subset_images[i]\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    # Standard CPU Preprocessing\n",
    "    inputs = processor(text=candidate_queries, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    pre_proc_times.append(get_ms(t0, t1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t2 = time.perf_counter()\n",
    "        outputs = model(**inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        t3 = time.perf_counter()\n",
    "        model_times.append(get_ms(t2, t3))\n",
    "    \n",
    "    # Post-processing\n",
    "    t4 = time.perf_counter()\n",
    "    probs = outputs.logits_per_image.softmax(dim=1)\n",
    "    predicted_id = probs.argmax().item()\n",
    "    t5 = time.perf_counter()\n",
    "    post_proc_times.append(get_ms(t4, t5))\n",
    "\n",
    "print(f\"Average Latency (CPU Pre-proc): {sum(pre_proc_times)/50:.2f} ms\")\n",
    "print(f\"Average Latency (Model): {sum(model_times)/50:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bench-gpu-title",
   "metadata": {},
   "source": [
    "## 5. Optimized Case: GPU-Accelerated Preprocessing\n",
    "By using `torchvision.transforms` directly on GPU tensors, we can significantly reduce the latency of the preprocessing step, especially when handling batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-preproc-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "# CLIP-specific normalization\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "gpu_preprocess = T.Compose([\n",
    "    T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(224),\n",
    "    T.ConvertImageDtype(torch.float32),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "def benchmark_gpu_preproc():\n",
    "    # Warm up\n",
    "    torch.cuda.synchronize()\n",
    "    start_gpu = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert images to tensors and move to GPU\n",
    "        raw_tensors = torch.stack([T.functional.to_tensor(img) for img in subset_images]).to(device)\n",
    "        # Parallel GPU preprocessing\n",
    "        _ = gpu_preprocess(raw_tensors)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    return (time.perf_counter() - start_gpu) * 1000\n",
    "\n",
    "gpu_time = benchmark_gpu_preproc()\n",
    "print(f\"GPU Preprocessing (50 images Batch): {gpu_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "As we've seen, moving preprocessing to the GPU can lead to substantial speedups. \n",
    "\n",
    "### Future Optimizations:\n",
    "- **NVIDIA DALI:** For even more advanced pipeline parallelization.\n",
    "- **TensorRT Deployment:** Compiling the model into a high-performance engine for production use in Triton server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
